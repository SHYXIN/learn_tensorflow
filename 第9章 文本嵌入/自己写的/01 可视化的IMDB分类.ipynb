{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "4.5.2\n",
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\WX847\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 80/80 [00:16<00:00,  4.93 MiB/s]rl]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:16<00:00, 16.23s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\WX847\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tfds.__version__)\n",
    "\n",
    "imdb, info =tfds.load('imdb_reviews', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msplit\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AbstractSplit'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdata_dir\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mshuffle_files\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mas_supervised\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdecoders\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorflow_datasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PartialDecoding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TreeDict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mread_config\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorflow_datasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mwith_info\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbuilder_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mas_dataset_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtry_gcs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Loads the named dataset into a `tf.data.Dataset`.\n",
      "\n",
      "`tfds.load` is a convenience method that:\n",
      "\n",
      "1. Fetch the `tfds.core.DatasetBuilder` by name:\n",
      "\n",
      "   ```python\n",
      "   builder = tfds.builder(name, data_dir=data_dir, **builder_kwargs)\n",
      "   ```\n",
      "\n",
      "2. Generate the data (when `download=True`):\n",
      "\n",
      "   ```python\n",
      "   builder.download_and_prepare(**download_and_prepare_kwargs)\n",
      "   ```\n",
      "\n",
      "3. Load the `tf.data.Dataset` object:\n",
      "\n",
      "   ```python\n",
      "   ds = builder.as_dataset(\n",
      "       split=split,\n",
      "       as_supervised=as_supervised,\n",
      "       shuffle_files=shuffle_files,\n",
      "       read_config=read_config,\n",
      "       decoders=decoders,\n",
      "       **as_dataset_kwargs,\n",
      "   )\n",
      "   ```\n",
      "\n",
      "See: https://www.tensorflow.org/datasets/overview#load_a_dataset for more\n",
      "examples.\n",
      "\n",
      "If you'd like NumPy arrays instead of `tf.data.Dataset`s or `tf.Tensor`s,\n",
      "you can pass the return value to `tfds.as_numpy`.\n",
      "\n",
      "**Warning**: calling this function might potentially trigger the download\n",
      "of hundreds of GiB to disk. Refer to the `download` argument.\n",
      "\n",
      "Args:\n",
      "  name: `str`, the registered name of the `DatasetBuilder` (the snake case\n",
      "    version of the class name). The config and version can also be specified\n",
      "    in the name as follows: `'dataset_name[/config_name][:version]'`. For\n",
      "      example, `'movielens/25m-ratings'` (for the latest version of\n",
      "    `'25m-ratings'`), `'movielens:0.1.0'` (for the default config and\n",
      "    version 0.1.0), or`'movielens/25m-ratings:0.1.0'`. Note that only the\n",
      "      latest version can be generated, but old versions can be read if they\n",
      "      are present on disk. For convenience, the `name` parameter can contain\n",
      "      comma-separated keyword arguments for the builder. For example,\n",
      "      `'foo_bar/a=True,b=3'` would use the `FooBar` dataset passing the\n",
      "      keyword arguments `a=True` and `b=3` (for builders with configs, it\n",
      "      would be `'foo_bar/zoo/a=True,b=3'` to use the `'zoo'` config and pass\n",
      "      to the builder keyword arguments `a=True` and `b=3`).\n",
      "  split: Which split of the data to load (e.g. `'train'`, `'test'`,\n",
      "    `['train', 'test']`, `'train[80%:]'`,...). See our\n",
      "    [split API guide](https://www.tensorflow.org/datasets/splits). If `None`,\n",
      "      will return all splits in a `Dict[Split, tf.data.Dataset]`\n",
      "  data_dir: `str`, directory to read/write data. Defaults to the value of the\n",
      "    environment variable TFDS_DATA_DIR, if set, otherwise falls back to\n",
      "    '~/tensorflow_datasets'.\n",
      "  batch_size: `int`, if set, add a batch dimension to examples. Note that\n",
      "    variable length features will be 0-padded. If `batch_size=-1`, will return\n",
      "    the full dataset as `tf.Tensor`s.\n",
      "  shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
      "    `False`.\n",
      "  download: `bool` (optional), whether to call\n",
      "    `tfds.core.DatasetBuilder.download_and_prepare` before calling\n",
      "    `tf.DatasetBuilder.as_dataset`. If `False`, data is expected to be in\n",
      "    `data_dir`. If `True` and the data is already in `data_dir`,\n",
      "    `download_and_prepare` is a no-op.\n",
      "  as_supervised: `bool`, if `True`, the returned `tf.data.Dataset` will have a\n",
      "    2-tuple structure `(input, label)` according to\n",
      "    `builder.info.supervised_keys`. If `False`, the default, the returned\n",
      "    `tf.data.Dataset` will have a dictionary with all the features.\n",
      "  decoders: Nested dict of `Decoder` objects which allow to customize the\n",
      "    decoding. The structure should match the feature structure, but only\n",
      "    customized feature keys need to be present. See [the\n",
      "      guide](https://github.com/tensorflow/datasets/tree/master/docs/decode.md)\n",
      "        for more info.\n",
      "  read_config: `tfds.ReadConfig`, Additional options to configure the input\n",
      "    pipeline (e.g. seed, num parallel reads,...).\n",
      "  with_info: `bool`, if `True`, `tfds.load` will return the tuple\n",
      "    (`tf.data.Dataset`, `tfds.core.DatasetInfo`), the latter containing the\n",
      "    info associated with the builder.\n",
      "  builder_kwargs: `dict` (optional), keyword arguments to be passed to the\n",
      "    `tfds.core.DatasetBuilder` constructor. `data_dir` will be passed through\n",
      "    by default.\n",
      "  download_and_prepare_kwargs: `dict` (optional) keyword arguments passed to\n",
      "    `tfds.core.DatasetBuilder.download_and_prepare` if `download=True`. Allow\n",
      "    to control where to download and extract the cached data. If not set,\n",
      "    cache_dir and manual_dir will automatically be deduced from data_dir.\n",
      "  as_dataset_kwargs: `dict` (optional), keyword arguments passed to\n",
      "    `tfds.core.DatasetBuilder.as_dataset`.\n",
      "  try_gcs: `bool`, if True, tfds.load will see if the dataset exists on the\n",
      "    public GCS bucket before building it locally.\n",
      "\n",
      "Returns:\n",
      "  ds: `tf.data.Dataset`, the dataset requested, or if `split` is None, a\n",
      "    `dict<key: tfds.Split, value: tf.data.Dataset>`. If `batch_size=-1`,\n",
      "    these will be full datasets as `tf.Tensor`s.\n",
      "  ds_info: `tfds.core.DatasetInfo`, if `with_info` is True, then `tfds.load`\n",
      "    will return a tuple `(ds, ds_info)` containing dataset information\n",
      "    (version, features, splits, num_examples,...). Note that the `ds_info`\n",
      "    object documents the entire dataset, regardless of the `split` requested.\n",
      "    Split-specific information is available in `ds_info.splits`.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\wx847\\anaconda3\\envs\\learn_tensorflow\\lib\\site-packages\\tensorflow_datasets\\core\\load.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "?tfds.load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data, test_data = imdb['train'],imdb['test']\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []  \n",
    "testing_labels = []\n",
    "\n",
    "for s,l in train_data:\n",
    "  training_sentences.append(str(s.numpy()))\n",
    "  training_labels.append(l.numpy())\n",
    "  \n",
    "  \n",
    "for s,l in test_data:\n",
    "  testing_sentences.append(str(s.numpy()))\n",
    "  testing_labels.append(l.numpy())\n",
    " \n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m            EagerTensor\n",
      "\u001b[1;31mString form:\u001b[0m     tf.Tensor(b\"They just don't make cartoons like they used to. This one had wit, great characters,  <...> artoon shows, and one of the most honored, winning several Emmy Awards.\", shape=(), dtype=string)\n",
      "\u001b[1;31mFile:\u001b[0m            c:\\users\\wx847\\anaconda3\\envs\\learn_tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\n",
      "\u001b[1;31mDocstring:\u001b[0m       <no docstring>\n",
      "\u001b[1;31mClass docstring:\u001b[0m Base class for EagerTensor.\n"
     ]
    }
   ],
   "source": [
    "?s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m            EagerTensor\n",
      "\u001b[1;31mString form:\u001b[0m     tf.Tensor(1, shape=(), dtype=int64)\n",
      "\u001b[1;31mFile:\u001b[0m            c:\\users\\wx847\\anaconda3\\envs\\learn_tensorflow\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\n",
      "\u001b[1;31mDocstring:\u001b[0m       <no docstring>\n",
      "\u001b[1;31mClass docstring:\u001b[0m Base class for EagerTensor.\n"
     ]
    }
   ],
   "source": [
    "?l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trunc_type = 'post'  # 截断方式，后边的被丢弃\n",
    "oov_tok = '<OOV>'  # 未登录词语\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 训练集，进行词条化，然后进行序列化\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences,maxlen=max_length,truncating=trunc_type,padding='post')\n",
    "\n",
    "# 测试集，用训练集的tokenizer词条化, 然后进行序列化即可\n",
    "testing_sequences= tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查一下编的对不对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <OOV> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <OOV> without any real concern for anything else i cant recommend this film at all ' ? ? ? ? ? ? ?\n",
      "b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = {v:k for k,v in word_index.items()}  # 反转\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i,'?') for i in text])\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000   # 最大的词语数量\n",
    "embedding_dim = 16  # 每个word都embedding为一个长度为16的向量\n",
    "max_length = 120  # 句子的最长\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 7s 261us/sample - loss: 0.4879 - accuracy: 0.7513 - val_loss: 0.3621 - val_accuracy: 0.8396\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 5s 198us/sample - loss: 0.2464 - accuracy: 0.9042 - val_loss: 0.3726 - val_accuracy: 0.8390\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 5s 195us/sample - loss: 0.1051 - accuracy: 0.9720 - val_loss: 0.4636 - val_accuracy: 0.8206\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 5s 215us/sample - loss: 0.0284 - accuracy: 0.9957 - val_loss: 0.5510 - val_accuracy: 0.8126\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 5s 204us/sample - loss: 0.0071 - accuracy: 0.9996 - val_loss: 0.6144 - val_accuracy: 0.8188\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 5s 200us/sample - loss: 0.0023 - accuracy: 0.9999 - val_loss: 0.6764 - val_accuracy: 0.8180\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 5s 216us/sample - loss: 9.3676e-04 - accuracy: 1.0000 - val_loss: 0.7300 - val_accuracy: 0.8165\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 5s 202us/sample - loss: 5.0250e-04 - accuracy: 1.0000 - val_loss: 0.7718 - val_accuracy: 0.8167\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 5s 209us/sample - loss: 2.8799e-04 - accuracy: 1.0000 - val_loss: 0.8115 - val_accuracy: 0.8172\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 5s 198us/sample - loss: 1.7108e-04 - accuracy: 1.0000 - val_loss: 0.8494 - val_accuracy: 0.8182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2234ba8a048>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "# 将上边处理好的序列，添加到模型进行训练\n",
    "model.fit(padded,training_labels_final,epochs=num_epochs, validation_data=(testing_padded, testing_labels_final)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看第一层embedding的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e =model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape)  # shape: (vocab_size, embedding_dim)  # 词汇表的大小，每个word都embedding为一个长度为16的向量"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23b34fee034e6c6559cf1b732e166f57ae608166b2226a83e90a4ecbc0876e39"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
